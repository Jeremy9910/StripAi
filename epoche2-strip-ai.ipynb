{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e089f979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T08:41:19.247264Z",
     "iopub.status.busy": "2022-08-03T08:41:19.245456Z",
     "iopub.status.idle": "2022-08-03T08:42:06.925248Z",
     "shell.execute_reply": "2022-08-03T08:42:06.924245Z"
    },
    "papermill": {
     "duration": 47.687889,
     "end_time": "2022-08-03T08:42:06.928172",
     "exception": false,
     "start_time": "2022-08-03T08:41:19.240283",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hugsvision\r\n",
      "  Downloading hugsvision-0.75.3-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from hugsvision) (9.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from hugsvision) (1.0.2)\r\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (from hugsvision) (0.9.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from hugsvision) (4.64.0)\r\n",
      "Collecting timm\r\n",
      "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.7/site-packages (from hugsvision) (1.6.5)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from hugsvision) (1.11.0)\r\n",
      "Collecting pycocotools\r\n",
      "  Downloading pycocotools-2.0.4.tar.gz (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from hugsvision) (0.12.0)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from hugsvision) (0.8.10)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from hugsvision) (4.20.1)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from hugsvision) (3.5.2)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from hugsvision) (4.5.4.60)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (1.21.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (2.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (21.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (1.4.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (4.33.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->hugsvision) (3.0.9)\r\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->hugsvision) (0.3.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->hugsvision) (6.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->hugsvision) (3.19.4)\r\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->hugsvision) (2022.5.0)\r\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->hugsvision) (2.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->hugsvision) (4.1.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->hugsvision) (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->hugsvision) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->hugsvision) (3.1.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->hugsvision) (2.28.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->hugsvision) (3.7.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->hugsvision) (4.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers->hugsvision) (0.8.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->hugsvision) (2021.11.10)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->hugsvision) (0.12.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (3.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->hugsvision) (1.16.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (3.3.7)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (1.35.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (0.4.6)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (59.8.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (1.1.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (0.37.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (2.1.2)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (1.43.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning->hugsvision) (1.8.1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->hugsvision) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->hugsvision) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->hugsvision) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->hugsvision) (1.26.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->hugsvision) (3.8.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->hugsvision) (4.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->hugsvision) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->hugsvision) (0.2.7)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->hugsvision) (1.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (1.7.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (1.3.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (6.0.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (21.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (1.2.0)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (0.13.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->hugsvision) (4.0.2)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->hugsvision) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->hugsvision) (3.2.0)\r\n",
      "Building wheels for collected packages: pycocotools\r\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp37-cp37m-linux_x86_64.whl size=370081 sha256=6590b1d2dd4fa964d53a6e2a9e90fff575de88d06fff53de2a5253b8b78cf145\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/5f/fa/f011e578cc76e1fc5be8dce30b3eb9fd00f337e744b3bba59b\r\n",
      "Successfully built pycocotools\r\n",
      "Installing collected packages: timm, pycocotools, hugsvision\r\n",
      "Successfully installed hugsvision-0.75.3 pycocotools-2.0.4 timm-0.6.7\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "# Install Environment dependencies\n",
    "!pip install hugsvision\n",
    "# DONT LOAD WHAT IS NOT NEEDED\n",
    "#!pip install pyvips\n",
    "#!sudo apt-get -y install lynx\n",
    "#!sudo apt-get -y install w3m\n",
    "#!sudo apt-get install desktop-file-utils\n",
    "\n",
    "#import modules\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from hugsvision.dataio.VisionDataset import VisionDataset\n",
    "from torchvision.transforms import InterpolationMode\n",
    "InterpolationMode.BICUBIC\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ONLY IMPORT WHAT IS NEEDED\n",
    "#import tensorflow as tf\n",
    "#from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "#import cv2 as cv\n",
    "#import PIL\n",
    "#import numpy as np\n",
    "#import torch\n",
    "#import glob\n",
    "#from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208a70af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T08:42:06.939378Z",
     "iopub.status.busy": "2022-08-03T08:42:06.938213Z",
     "iopub.status.idle": "2022-08-03T08:42:07.101652Z",
     "shell.execute_reply": "2022-08-03T08:42:07.100640Z"
    },
    "papermill": {
     "duration": 0.170834,
     "end_time": "2022-08-03T08:42:07.103807",
     "exception": false,
     "start_time": "2022-08-03T08:42:06.932973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 754 entries, 0 to 753\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   image_id    754 non-null    object \n",
      " 1   center_id   754 non-null    int64  \n",
      " 2   patient_id  754 non-null    object \n",
      " 3   image_num   754 non-null    int64  \n",
      " 4   label       754 non-null    object \n",
      " 5   file_size   754 non-null    float64\n",
      " 6   group       754 non-null    int64  \n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 41.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train1 = pd.read_csv('../input/mayo-clinic-1024-jpg-part1/train_with_groups.csv')\n",
    "df_train2 = pd.read_csv('../input/mayo-clinic-1024-jpg-part2-1/train_with_groups.csv')\n",
    "df_train3 = pd.read_csv('../input/mayo-clinic-1024-jpg-part3/train_with_groups.csv')\n",
    "df_train4 = pd.read_csv('../input/mayo-clinic-1024-jpg-part4-1/train_with_groups.csv')\n",
    "df_train5 = pd.read_csv('../input/mayo-clinic-1024-jpg-part5-1/train_with_groups.csv')\n",
    "df_train6 = pd.read_csv('../input/mayo-clinic-1024-jpg-part6/train_with_groups.csv')\n",
    "df_train7 = pd.read_csv('../input/mayo-clinic-1024-jpg-part7-1/train_with_groups.csv')\n",
    "df_train8 = pd.read_csv('../input/mayo-clinic-1024-jpg-part8/train_with_groups.csv')\n",
    "df_train9 = pd.read_csv('../input/mayo-clinic-1024-jpg-part9/train_with_groups.csv')\n",
    "df_train10 = pd.read_csv('../input/mayo-clinic-1024-jpg-part10/train_with_groups.csv')\n",
    "\n",
    "df_train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce4d9c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T08:42:07.113966Z",
     "iopub.status.busy": "2022-08-03T08:42:07.113688Z",
     "iopub.status.idle": "2022-08-03T08:42:07.120450Z",
     "shell.execute_reply": "2022-08-03T08:42:07.119585Z"
    },
    "papermill": {
     "duration": 0.013922,
     "end_time": "2022-08-03T08:42:07.122397",
     "exception": false,
     "start_time": "2022-08-03T08:42:07.108475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define relevant Paths\n",
    "#train_source_path1 = Path('../input/mayo-clinic-1024-jpg-part1/train')\n",
    "#train_source_path2 = Path('../input/mayo-clinic-1024-jpg-part2-1/train')\n",
    "train_source_paths = [Path(i) / 'train' for i in Path('../input').glob('./mayo-clinic-1024*')]\n",
    "train_dest_path = Path('../working/data')\n",
    "ce_train_path = Path('../working/data/CE')\n",
    "laa_train_path = Path('../working/data/LAA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf37316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T08:42:07.133323Z",
     "iopub.status.busy": "2022-08-03T08:42:07.131902Z",
     "iopub.status.idle": "2022-08-03T08:42:07.137600Z",
     "shell.execute_reply": "2022-08-03T08:42:07.136720Z"
    },
    "papermill": {
     "duration": 0.012886,
     "end_time": "2022-08-03T08:42:07.139496",
     "exception": false,
     "start_time": "2022-08-03T08:42:07.126610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = [1.9333333333333333e-05,\n",
    "1.866666666666667e-05,\n",
    "1.8000000000000004e-05,\n",
    "1.7333333333333336e-05,\n",
    "1.6666666666666667e-05,\n",
    "1.6000000000000003e-05,\n",
    "1.5333333333333334e-05,\n",
    "1.4666666666666668e-05,\n",
    "1.4000000000000001e-05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bdb2fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T08:42:07.150864Z",
     "iopub.status.busy": "2022-08-03T08:42:07.149481Z",
     "iopub.status.idle": "2022-08-03T17:24:58.731053Z",
     "shell.execute_reply": "2022-08-03T17:24:58.729868Z"
    },
    "papermill": {
     "duration": 31371.590058,
     "end_time": "2022-08-03T17:24:58.733987",
     "exception": false,
     "start_time": "2022-08-03T08:42:07.143929",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Datasets...\n",
      "train_ds:  52331\n",
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 28535 | 23796 | 52331 |\n",
      "|  Test   | 5059  | 4177  | 9236  |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045e0d3a2c894c8a8193b2ff520d834d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 52331\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1636' max='1636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1636/1636 38:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.424202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9236\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  46731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../input/model-new/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 32516 | 14215 | 46731 |\n",
      "|  Test   | 5738  | 2509  | 8247  |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 46731\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1461' max='1461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1461/1461 33:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.390229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8247\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  61693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-09-05-13/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 42128 | 19565 | 61693 |\n",
      "|  Test   | 7497  | 3391  | 10888 |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 61693\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1928' max='1928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1928/1928 44:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>0.428834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10888\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  54677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-10-04-10/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 38622 | 16055 | 54677 |\n",
      "|  Test   | 6788  | 2861  | 9649  |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 54677\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1709' max='1709' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1709/1709 39:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.349682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9649\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  51076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-11-04-07/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 36160 | 14916 | 51076 |\n",
      "|  Test   | 6341  | 2673  | 9014  |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 51076\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1597' max='1597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1597/1597 37:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.373566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9014\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  18497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-12-12-32/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+-------+\n",
      "| Dataset |  CE   | LAA  | Total |\n",
      "+---------+-------+------+-------+\n",
      "|  Train  | 14259 | 4238 | 18497 |\n",
      "|  Test   | 2514  | 751  | 3265  |\n",
      "+---------+-------+------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 18497\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='579' max='579' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [579/579 13:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.106210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3265\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  56864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-13-15-12/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 38437 | 18427 | 56864 |\n",
      "|  Test   | 6879  | 3156  | 10035 |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 56864\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1777' max='1777' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1777/1777 41:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.420020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10035\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  11693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-14-00-32/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+\n",
      "| Dataset |  CE  | LAA  | Total |\n",
      "+---------+------+------+-------+\n",
      "|  Train  | 5003 | 6690 | 11693 |\n",
      "|  Test   | 936  | 1128 | 2064  |\n",
      "+---------+------+------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 11693\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='366' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [366/366 08:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.431381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2064\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  54360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-14-40-11/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 41961 | 12399 | 54360 |\n",
      "|  Test   | 7387  | 2207  | 9594  |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 54360\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1699' max='1699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1699/1699 39:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.379551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9594\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40\u001b[0m\n",
      "Split Datasets...\n",
      "train_ds:  40154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/model/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"../working/out/MAYOCLINICMODEL/1_2022-08-03-15-26-26/model\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"CE\",\n",
      "    \"1\": \"LAA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"CE\": \"0\",\n",
      "    \"LAA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+\n",
      "| Dataset |  CE   |  LAA  | Total |\n",
      "+---------+-------+-------+-------+\n",
      "|  Train  | 27420 | 12734 | 40154 |\n",
      "|  Test   | 4839  | 2247  | 7086  |\n",
      "+---------+-------+-------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at ../working/out/MAYOCLINICMODEL/1_2022-08-03-15-59-40/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 40154\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'CE', '1': 'LAA'}\n",
      "{'CE': '0', 'LAA': '1'}\n",
      "Trainer builded!\n",
      "Start Training!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1255' max='1255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1255/1255 28:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.371046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7086\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36/trainer/\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36/trainer/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36/trainer/pytorch_model.bin\n",
      "Configuration saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36/model/config.json\n",
      "Model weights saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36/model/pytorch_model.bin\n",
      "Feature extractor saved in ../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36/feature_extractor/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: \u001b[93m../working/out/MAYOCLINICMODEL/1_2022-08-03-16-56-36\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# only take first 1k images REMOVE IN PRODUCTION\n",
    "i = 0 \n",
    "from hugsvision.nnet.VisionClassifierTrainer import VisionClassifierTrainer\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "old_model = Path('../input/model-new/model')\n",
    "huggingface_model = 'google/vit-base-patch16-224-in21k'\n",
    "for source_path in train_source_paths:\n",
    "    #setting up enviroment\n",
    "    # clean working directory\n",
    "    if os.path.isdir(train_dest_path):\n",
    "        shutil.rmtree(train_dest_path)\n",
    "        os.makedirs(train_dest_path)\n",
    "    else:\n",
    "        os.makedirs(train_dest_path)\n",
    "        \n",
    "    # create one folder for each image class    \n",
    "    os.mkdir(ce_train_path)\n",
    "    os.mkdir(laa_train_path)\n",
    "    \n",
    "    # set df\n",
    "    df_train = pd.read_csv(source_path / '../train_with_groups.csv')\n",
    "    \n",
    "    for img in list(source_path.glob('./*.jpg')):\n",
    "        image_id = img.stem[:8]\n",
    "        image_label = df_train[df_train['image_id'] == image_id].iloc[0][\"label\"]\n",
    "        shutil.copyfile(img, train_dest_path / image_label / img.name)\n",
    "        \n",
    "    train, test, id2label, label2id = VisionDataset.fromImageFolder('/kaggle/working/data/', balanced=False)\n",
    "    if i == 0:\n",
    "        trainer = VisionClassifierTrainer(\n",
    "            model_name   = \"MayoClinicModel\",\n",
    "            train        =  train,\n",
    "            test         =  test,\n",
    "            output_dir   = \"../working/out/\",\n",
    "            max_epochs   = 1,\n",
    "            batch_size   = 32, \n",
    "            lr       = 2e-5,\n",
    "            fp16     = False,\n",
    "            model = ViTForImageClassification.from_pretrained(\n",
    "                old_model,\n",
    "                num_labels = len(label2id),\n",
    "                label2id   = label2id,\n",
    "                id2label   = id2label\n",
    "            ),\n",
    "            feature_extractor = ViTFeatureExtractor.from_pretrained(huggingface_model),\n",
    "        )\n",
    "        i = 1\n",
    "    else:\n",
    "        current_model =max(glob.glob(os.path.join('../working/out/MAYOCLINICMODEL/', '*/')), key=os.path.getmtime)\n",
    "        model_path_source = Path('kaggle/working/out/MAYOCLINICMODEL').glob( '../model')\n",
    "        trainer = VisionClassifierTrainer(\n",
    "            model_name   = \"MayoClinicModel\",\n",
    "            train        =  train,\n",
    "            test         =  test,\n",
    "            output_dir   = \"../working/out/\",\n",
    "            max_epochs   = 1,\n",
    "            batch_size   = 32, \n",
    "            lr       = lr[i-1],\n",
    "            fp16     = False,\n",
    "            model = ViTForImageClassification.from_pretrained(\n",
    "                current_model +  'model',\n",
    "                num_labels = len(label2id),\n",
    "                label2id   = label2id,\n",
    "                id2label   = id2label\n",
    "            ),\n",
    "            feature_extractor = ViTFeatureExtractor.from_pretrained(huggingface_model),\n",
    "        )\n",
    "        i += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31430.307856,
   "end_time": "2022-08-03T17:25:02.013147",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-03T08:41:11.705291",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "045e0d3a2c894c8a8193b2ff520d834d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4b184ef12bef4399b4023c26b7927e1b",
        "IPY_MODEL_f7873ed8dc01431b846a4ab32f82e96b",
        "IPY_MODEL_bad1f6edecc54fe0bf1b8810a6595735"
       ],
       "layout": "IPY_MODEL_54b1d39bb6194baba3399984ba64249b"
      }
     },
     "04868600900342c78d5b8f616c83602b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b184ef12bef4399b4023c26b7927e1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8f9f5176859148fb8efc37cb9205e6f4",
       "placeholder": "​",
       "style": "IPY_MODEL_985c5f4c1e8f4444b7fa06646d3f05fa",
       "value": "Downloading: 100%"
      }
     },
     "54b1d39bb6194baba3399984ba64249b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f9f5176859148fb8efc37cb9205e6f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "985c5f4c1e8f4444b7fa06646d3f05fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "995a37301b964333a775cb43bb8da82a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9a91f57bd9fc4e7b8cadf5172bd3c774": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bad1f6edecc54fe0bf1b8810a6595735": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_04868600900342c78d5b8f616c83602b",
       "placeholder": "​",
       "style": "IPY_MODEL_995a37301b964333a775cb43bb8da82a",
       "value": " 160/160 [00:00&lt;00:00, 4.09kB/s]"
      }
     },
     "e0c62210e5284b8a8a619142cae6576f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f7873ed8dc01431b846a4ab32f82e96b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a91f57bd9fc4e7b8cadf5172bd3c774",
       "max": 160.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e0c62210e5284b8a8a619142cae6576f",
       "value": 160.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
